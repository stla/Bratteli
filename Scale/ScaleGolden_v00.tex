\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}

\usepackage{diagbox}

\usepackage{enumerate}

%\usepackage{array}
%\usepackage{makecell}
%\usepackage{tikz}
%\newcommand\diag[4]{%
%  \multicolumn{1}{p{#2}|}{\hskip-\tabcolsep
%  $\vcenter{\begin{tikzpicture}[baseline=0,anchor=south west,inner sep=#1]
%  \path[use as bounding box] (0,0) rectangle (#2+2\tabcolsep,\baselineskip);
%  \node[minimum width={#2+2\tabcolsep},minimum height=\baselineskip+\extrarowheight] (box) {};
%  \draw (box.north west) -- (box.south east);
%  \node[anchor=south west] at (box.south west) {#3};
%  \node[anchor=north east] at (box.north east) {#4};
% \end{tikzpicture}}$\hskip-\tabcolsep}}

\author{Stéphane Laurent}
\title{xxx}
\begin{document}


\newtheoremstyle{thmstyle}{3pt}{3pt}{\itshape}{}{\bf}{.}{.5em}{}      
\newtheoremstyle{defstyle}{3pt}{3pt}{\sffamily}{}{\bf}{.}{.5em}{} 
\theoremstyle{defstyle}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{question}{Question}
\newtheorem{clarify}{To clarify}
\theoremstyle{thmstyle}
\newtheorem{thm}{Theorem}[section]
\newtheorem{ppsition}{Proposition}
\newtheorem{lemma}{Lemma}

\newcommand{\BB}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\II}{\mathcal{I}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\given}{\mid}
\newcommand{\eps}{\epsilon}
\newcommand{\indic}{\boldsymbol 1}
\newcommand{\Vb}{\boldsymbol V}
\newcommand{\tildV}{\widetilde{V}}

\newcommand{\indvee}{\dot{\vee}}
\newcommand{\indep}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}


\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Joinable processes}

When two random vectors 
${(X_n)}_{0 \leq n \leq n_0}$ and ${(Y_n)}_{0 \leq n \leq n_0}$ are defined 
on the same probability space and the filtrations they generate are 
jointly immersed (see below), we will write and say that 
$\left\{\begin{smallmatrix} {(X_n)}_{0 \leq n \leq n_0} \\ 
{(Y_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$  is a \emph{synchronous joining}. 
That means, after introducing the $\sigma$-fields 
$\BB_n = \sigma(X_{0}, \ldots, X_n)$ and  
$\CC_n = \sigma(Y_{0}, \ldots, Y_n)$, that 
$\BB_{n+1} \indep_{\BB_n} \BB_n \vee \CC_n$ 
and $\CC_{n+1} \indep_{\CC_n} \BB_n \vee \CC_n$.  

When $\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(Y'_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$  is a \emph{synchronous joining} in the situation when 
${(X'_n)}_{0 \leq n \leq n_0}$ is a copy of a random vector 
${(X_n)}_{0 \leq n \leq n_0}$  and ${(Y'_n)}_{0 \leq n \leq n_0}$ 
is a copy of a random vector ${(Y_n)}_{0 \leq n \leq n_0}$, we also say that 
$\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(Y'_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$  is a \emph{synchronous joining of 
${(X_n)}_{0 \leq n \leq n_0}$ and ${(Y_n)}_{0 \leq n \leq n_0}$}. 

\begin{definition}
Let ${(X_n)}_{n \leq 0}$ and ${(Y_n)}_{n \leq 0}$ be two stochastic processes. 
We say that they are \emph{closely joinable} if for every $\delta >0$ there exists 
$N_\delta \leq 0$ such that for every $n_0 \leq N_\delta$ there exists a 
synchronous joining $\left\{\begin{smallmatrix} {(X'_n)}_{n_0 \leq n \leq N_\delta} \\ 
{(Y'_n)}_{n_0 \leq n \leq N_\delta}
\end{smallmatrix}\right.$ 
of ${(X_n)}_{n_0 \leq n \leq N_\delta}$ and ${(Y_n)}_{n_0 \leq n \leq N_\delta}$ 
such that 
$$
\Pr(X'_{n_0}=Y'_{n_0}, \ldots, X'_{N_\delta}=Y'_{N_\delta}) > 1-\delta. 
$$ 
\end{definition}

\begin{thm}\label{thm:joinable}
Let ${(X_n)}_{n \leq 0}$ and ${(Y_n)}_{n \leq 0}$ be two Markovian stochastic processes, and 
denote by $\FF$ and $\GG$ the filtrations they respectively generate. 
Assume ${(X_n)}_{n \leq 0}$ and ${(Y_n)}_{n \leq 0}$ are closely joinable. 
\begin{enumerate}
\item The $\sigma$-field $\FF_{-\infty}$ is degenerate if and 
only if the $\sigma$-field $\GG_{-\infty}$ is degenerate.

\item The filtration $\FF$ is I-cosy if and only if the filtration $\GG$ is I-cosy.
\end{enumerate}
\end{thm}

We firstly prove the first point of this theorem. 

\begin{proof}[Proof of 1 in Theorem~\ref{thm:joinable}]
Let $X_0$ be a $\FF_0$-measurable random variable taking its values in $[0,1]$. 
To establish the claim, it suffices to show that 
$\EE[X_{0} \given \FF_{-\infty}]$ can be made as $L^1$-close as desired to a 
$\GG_{-\infty}$-measurable random variable. 

Let $\delta>0$ and $N = N_\delta$ the integer provided by the joinability assumption. 
One has 
$\EE[X_0 \given \FF_{N}\bigr] = g(X_{N})$ 
for a certain Borelian function $g$ taking its values in $[0,1]$. 
Take the joining provided by the joinability assumption with 
$n_0$ small enough in order that 
$$
\EE\left[ \Bigr| 
\EE[X_0 \given \FF_{n_0}] - \EE[X_0 \given \FF_{-\infty}] 
\Bigl| \right] 
\leq \delta
\quad \text{and }\; 
\EE\left[ \Bigr| 
\EE\bigl[g(Y_N) \given \GG_{n_0}\bigr] - \EE\bigl[g(Y_N) \given \GG_{-\infty}\bigr]
\Bigl| \right] 
\leq \delta,
$$
which is possibly by virtue of the theorem on reverse martingale convergence. 

One has $\EE\bigl[\bigl|g(X'_N) - g(Y'_N)\bigr|\Bigr] \leq \delta$. Therefore 
$$
\EE\left[ \Bigr|\EE\bigl[g(X'_N) \given \sigma(X'_{n_0}, Y'_{n_0})\bigr] 
 - \EE\bigl[g(Y'_N) \given \sigma(X'_{n_0}, Y'_{n_0}) \bigr] \Bigl| \right] \leq \delta.
$$
because of the contractivity of the conditional expectation. 
By immersion, 
$$
\EE\bigl[g(X'_N) \given \sigma(X'_{n_0}, Y'_{n_0})\bigr] 
= \EE\bigl[g(X'_N) \given \sigma(X'_{n_0})\bigr] = \EE\bigl[X'_0 \given \sigma(X'_{n_0})\bigr]
$$
and 
$$
\EE\bigl[g(Y'_N) \given \sigma(X'_{n_0}, Y'_{n_0})\bigr] 
= \EE\bigl[g(Y'_N) \given \sigma(Y'_{n_0})\bigr]. 
$$
On the other hand, 
\begin{align*}
\EE\left[ \Bigr| 
\EE\bigl[X'_0 \given \sigma(X'_{n_0})\bigr] 
 - \EE\bigl[g(Y'_N) \given \sigma(Y'_{n_0})\bigr] 
\Bigl| \right] 
& = \EE\left[ \Bigr| 
\EE\bigl[X_0 \given \sigma(X_{n_0})\bigr] 
 - \EE\bigl[g(Y_N) \given \sigma(Y_{n_0})\bigr] 
\Bigl| \right] \\
& = \EE\left[ \Bigr| 
\EE\bigl[X_0 \given \FF_{n_0}\bigr] 
 - \EE\bigl[g(Y_N) \given \GG_{n_0}\bigr] 
\Bigl| \right]
\end{align*}
By combining the previous equalities and inequalities, 
$$
\EE\left[ \Bigr| 
\EE[X_0 \given \FF_{-\infty}\bigr]  -  \EE\bigl[g(Y_N) \given \GG_{-\infty}\bigr]
\Bigl| \right] 
\leq 3\delta,
$$
%
%\bigskip
%
%xxxxxx
%
%Let $\delta >0$ and take the joining provided by the joinability assumption. 
%
%xxxxx je suppose que j'ai le joining de $-\infty$ à $N$ xxxxx
%
%One has 
%$\EE[X'_0 \given \FF'_{N_\delta}\bigr] = g(X'_{N_\delta})$ 
%for a certain Borelian function $g$ taking its values in $[0,1]$, and 
%$\EE\bigl[\bigl|g(X'_N) - g(Y'_N)\bigr|\Bigr] \leq \delta$. Therefore 
%$$
%\EE\left[ \Bigr|\EE\bigl[g(X'_N) \given \HH_n\bigr] 
% - \EE\bigl[g(Y'_N) \given \HH_n \bigr] \Bigl| \right] \leq \delta.
%$$
%for every $n \leq N$ because of the contractivity of the conditional expectation. 
%
%By immersion, 
%$$
%\EE\bigl[g(X'_N) \given \HH_n\bigr] = \EE\bigl[g(X'_N) \given \FF_n\bigr] = \EE[X'_0 \given \FF'_{n}]
%$$
%and 
%$$
%\EE\bigl[g(Y'_N) \given \HH_n\bigr] = \EE\bigl[g(Y'_N) \given \GG_n\bigr].
%$$
%By the theorem on reverse martingale convergence, 
%$$
%\EE\left[ \Bigr| 
%\EE[X'_0 \given \FF'_{n}] - \EE[X'_0 \given \FF'_{-\infty}] 
%\Bigl| \right] 
%\leq \delta
%\quad \text{and }\; 
%\EE\left[ \Bigr| 
%\EE\bigl[g(Y'_N) \given \GG_n\bigr] - \EE\bigl[g(Y'_N) \given \GG_{-\infty}\bigr]
%\Bigl| \right] 
%\leq \delta
%$$
%whenever $n$ is small enough. 
%By combining the previous equalities and inequalities, 
%$$
%\EE\left[ \Bigr| 
%\EE[X'_0 \given \FF'_{-\infty}\bigr]  -  \EE\bigl[g(Y'_N) \given \GG_{-\infty}\bigr]
%\Bigl| \right] 
%\leq 3\delta.
%$$
%
%
%\bigskip 
%
%xxxxxxxxxx
%
%On suppose $\GG_{-\infty}$ triviale. Montrons que $\FF_{-\infty}$ est triviale. 
%
%Prenons un entier $N_0 \leq 0$ et $\delta >0$. 
%Tâchons de montrer que $\EE\bigl[f(X_{N_0}) \given \FF_n\bigr] \to 0$ dans $L^1$, où $f$ est une fonction mesurable qui prend ses valeurs dans $[0,1]$. 
%
%Prenons $N=N_\delta \leq N_0$ tel que en faisant le couplage qu'on a vu, on a $\Pr(X_N \neq Y_N) < \delta$. Notons $\HH=\FF\vee\GG$ la filtration engendrée par les deux processus. En raison de la coïmmersion, on a 
%$$
%\EE\bigl[f(X_{N_0}) \given \FF_n\bigr] = \EE\bigl[f(X_{N_0}) \given \HH_n\bigr] 
%$$
%Prenons $n \leq N$. On a 
%$$
%\EE\bigl[f(X_{N_0}) \given \HH_n\bigr] = 
%\EE\Bigl[\EE\bigl[f(X_{N_0}) \given X_N\bigr] \given \HH_n \Bigr].
%$$
%On a une fonction $g$ à valeurs dans $[0,1]$ telle que $\EE\bigl[f(X_{N_0}) \given X_N\bigr] = g(X_N)$, et on a pris $N$ de sorte que $\Pr\bigl(g(X_N) \neq g(Y_N)\bigr) < \delta$. Par la propriété de contractivité de l'espérance conditionnelle, 
%$$
%\EE\left[ \Bigr|\EE\bigl[g(X_N) \given \HH_n\bigr] 
% - \EE\bigl[g(Y_N) \given \HH_n \bigr] \Bigl| \right] \leq 2\delta.
%$$
%
%xxxx
%
%a-t-on \emph{égalité} ? 
%
%en construisant une suite $(n_k)$ telle que 
%$$
%\EE\bigl[f(X_{N_0}) \given \FF_{n_k}\bigr] - \EE\bigl[f(X_{N_0}) \given \GG_{n_k}\bigr] \to 0
%$$
%?
%xxx
%
%Pour $n$ suffisamment petit, 
%$$
%\EE\left[ \Bigr|\EE\bigl[g(Y_N) \given \HH_n \bigr] 
%- \EE\bigl[g(Y_N) \bigr] \Bigl| \right] \leq \delta, 
%$$
%et finalement 
%$$
%\EE\left[ \Bigr|\EE\bigl[g(X_N) \given \HH_n\bigr] 
% - \EE\bigl[g(Y_N) \bigr] \Bigl| \right] \leq 3\delta.
%$$
%
%Ceci montre que $\EE\bigl[f(X_{N_0}) \given \FF_n\bigr]$ a une limite dans $L^1$ aussi proche qu'on veut d'une constante. 
and the proof is over.
\end{proof}

\begin{remark}
We did not entirely use the inequality 
$\Pr(X'_{n_0}=Y'_{n_0}, \ldots, X'_{N_\delta}=Y'_{N_\delta}) > 1-\delta$, but 
only $\Pr(X'_{N_\delta}=Y'_{N_\delta}) > 1-\delta$.
\end{remark}

The second point of the theorem will be proved with the help of the following lemma. 

\begin{lemma}
Let 
$\left\{\begin{smallmatrix} {(X_n)}_{0 \leq n \leq n_0} \\ 
{(Y_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$  
be a synchronous joining of two random vectors 
${(X_n)}_{0 \leq n \leq n_0}$ and ${(Y_n)}_{0 \leq n \leq n_0}$. 
If 
$\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(X''_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$
is a synchronous joining of two copies of ${(X_n)}_{0 \leq n \leq n_0}$ 
on some probability space,  
then, on an enlargement of this probability space, there 
exists a synchronous joining 
$\left\{\begin{smallmatrix} {(Y'_n)}_{0 \leq n \leq n_0} \\ 
{(Y''_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$
of two copies of ${(Y_n)}_{0 \leq n \leq n_0}$ 
such that  
$\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(Y'_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$ and 
$\left\{\begin{smallmatrix} {(X''_n)}_{0 \leq n \leq n_0} \\ 
{(Y''_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$
are two copies of 
$\left\{\begin{smallmatrix} {(X_n)}_{0 \leq n \leq n_0} \\ 
{(Y_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$.
\end{lemma}

\begin{proof}
In the proof, we will use the three following elementary facts about 
conditional independence:
\begin{enumerate}[{\it(i)}]
\item If $U \indep {\cal B} \subset {\cal A}$, 
then $X \indep_\BB {\cal A}$ for any $\sigma({\cal B}, U)$-measurable r.v.\ $X$ mesurable.

\item If $U \indep \sigma({\cal B}, X)$ then $X \indep_\BB \sigma({\cal B}, U)$.

\item If $\CC \subset \BB \subset {\cal A}$, then 
the two conditional independences $Y \indep_{\BB \vee \sigma(X)} {\cal A} \vee \sigma(X)$
and $X \indep_{\CC} {\cal A}$ imply $Y \indep_{\BB} {\cal A}$.
\end{enumerate}

On the probability space of $\left\{\begin{smallmatrix} {(X_n)}_{0 \leq n \leq n_0} \\ 
{(Y_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$, one can assume there exist some random variables 
$U_0$, $\ldots$, $U_{n_0}$ such that 
\begin{itemize}
\item[$\bullet$] $U_n \indep \bigl((X_0,U_0), \ldots, (X_{n-1}, U_{n-1}), X_n\bigr)$;

\item[$\bullet$] $Y_n = f_n\bigl((X_0,Y_0), \ldots, (X_{n-1}, Y_{n-1}), X_n, U_n\bigr)$   
for some Borelian functions $f_n$. 
\end{itemize}

We denote by $(\FF'_0, \ldots, \FF'_{n_0})$ the filtration generated by 
${(X'_n)}_{0 \leq n \leq n_0}$ and by $(\FF''_0, \ldots, \FF''_{n_0})$ 
the filtration generated by ${(X''_n)}_{0 \leq n \leq n_0}$. 

On the probability space of $\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(X''_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$, one can assume there are two copies 
$\mathbf{U}'=(U'_0, \ldots, U'_{n_0})$ and $\mathbf{U}''=(U''_0, \ldots, U''_{n_0})$ of 
$(U_0, \ldots, U_{n_0})$ such that $\mathbf{U}' \indep \mathbf{U}''$ and 
$(\mathbf{U}',\mathbf{U}'') \indep \FF'_{n_0} \vee \FF''_{n_0}$. 

We set $Y'_0 = f_0(X'_0, U'_0)$ and $Y''_0 = f_0(X''_0, U''_0)$, and we 
recursively set 
$$
Y'_n = f_n\bigl((X'_0,Y'_0), \ldots, (X'_{n-1}, Y'_{n-1}), X'_n, U'_n\bigr)
$$
and 
$$
Y''_n = f_n\bigl((X''_0,Y''_0), \ldots, (X''_{n-1}, Y''_{n-1}), X''_n, U''_n\bigr).
$$
In this way, it is clear that $\left\{\begin{smallmatrix} {(X'_n)}_{0 \leq n \leq n_0} \\ 
{(Y'_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$ and 
$\left\{\begin{smallmatrix} {(X''_n)}_{0 \leq n \leq n_0} \\ 
{(Y''_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$
are two copies of 
$\left\{\begin{smallmatrix} {(X_n)}_{0 \leq n \leq n_0} \\ 
{(Y_n)}_{0 \leq n \leq n_0}
\end{smallmatrix}\right.$.

We define the filtration $(\HH_0, \ldots, \HH_{n_0})$ by setting 
$$
\HH_n = (\FF'_n \vee \FF''_n) \vee \sigma\bigl((U'_0,U''_0), \ldots, (U'_n, U''_n) \bigr) 
\supset  \sigma\bigl((X'_0,Y'_0,X''_0, Y''_0), \ldots, (X'_n,Y'_n,X''_n, Y''_n) \bigr).
$$

By property {\it(ii)}, 
$$
X'_{n+1} \indep_{\FF'_n\vee\FF''_n} \HH_n 
\quad \text{and }\;
X''_{n+1} \indep_{\FF'_n\vee\FF''_n} \HH_n 
$$
and by the immersion property, 
\begin{equation}\label{eq:immX}
X'_{n+1} \indep_{\FF'_n} \HH_n 
\quad \text{and }\;
X''_{n+1} \indep_{\FF''_n} \HH_n 
\end{equation}
Now, we define the filtrations  $(\GG'_0, \ldots, \GG'_{n_0})$ 
and  $(\GG''_0, \ldots, \GG''_{n_0})$ by setting 
$$
\GG'_n = \sigma\bigl((X'_0,Y'_0), \ldots, (X'_n, Y'_n)\bigr) 
\quad \text{and }\;
\GG''_n = \sigma\bigl((X''_0,Y''_0), \ldots, (X''_n, Y''_n)\bigr) 
$$
By property {\it(i)}, 
$$
Y'_{n+1} \indep_{\GG'_n \vee \sigma(X'_{n+1})} \HH_n \vee \sigma(X'_{n+1})
\quad \text{and }\;
Y''_{n+1} \indep_{\GG''_n \vee \sigma(X''_{n+1})} \HH_n \vee \sigma(X''_{n+1})
$$
and by \eqref{eq:immX} and property {\it(iii)},
$$
Y'_{n+1} \indep_{\GG'_n} \HH_n 
\quad \text{and }\;
Y''_{n+1} \indep_{\GG''_n} \HH_n
$$, 
and finally by the immersion property 
$$
Y'_{n+1} \indep_{\sigma(Y'_0, \ldots, Y'_n)} \HH_n 
\quad \text{and }\;
Y''_{n+1} \indep_{\sigma(Y''_0, \ldots, Y''_n)} \HH_n.
$$
Thus, we have proved that the four filtrations generated by 
${(X'_n)}_{0 \leq n \leq n_0}$, ${(X''_n)}_{0 \leq n \leq n_0}$, 
${(Y'_n)}_{0 \leq n \leq n_0}$, and ${(Y''_n)}_{0 \leq n \leq n_0}$ 
are jointly immersed in the filtration ${(\HH_n)}_{0 \leq n \leq n_0}$.
\end{proof}

Now we prove the second point of Theorem~\ref{thm:joinable}.

\begin{proof}[Proof of 2 in Theorem~\ref{thm:joinable}]

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example 1: Markov chains on the golden graph} 

We use the following notations.

\begin{itemize}
\item  ${(f_n)}_{n \leq 0}$ is the reversed Fibonnaci sequence:
$$
f_0=f_{-1}=1, f_{-2}=2, f_{-3}=3, f_{-4}=5, \ldots,
$$
and we also set $f_1=0$. 

\item $\theta=\frac{1}{1+\phi}=\frac{1}{\phi^2}=1-\frac{1}{\phi}=\frac{2}{3+\sqrt{5}}$ is the irrational 
number with continued fraction expansion $[0,2,1,1,\ldots]$
\end{itemize}


\subsection{The Markov chain ${(V_n)}_{n \leq 0}$}

We consider the Markov chain ${(V_n)}_{n \leq 0}$ walking on the graph shown on Figure~\ref{fig:GoldenGraph}, whose law is given by:

\begin{itemize}
\item $V_n$ takes the value $0$ or $1$ and $\Pr(V_n=1) = \frac{f_n}{f_n + \phi f_{n-1}} =: p_n$ 
(in particular $\Pr(V_0=1)=\theta$)
%  {(-1)}^{n+1} f_n\bigl(f_{n+1} - f_{n-1} \theta\bigr)

\item The transition matrix from $V_{n}$ to $V_{n+1}$ is 
\begin{center}
\begin{tabular}{|c||c|c|}\hline
\diagbox{$V_{n}$}{$V_{n+1}$}
&\makebox[3em]{$0$}&\makebox[3em]{$1$}\\ \hline\hline
$0$ & $f_n/f_{n-1}$ & $f_{n+1}/f_{n-1}$\\ \hline
$1$ & $1$ & $0$\\ \hline
\end{tabular}
\end{center}
\end{itemize}


\begin{figure}[!h]
   \centering
\scalebox{0.95}{
   \begin{subfigure}[t]{0.47\textwidth}
   \centering
   	\includegraphics[scale=0.6]{figures/GoldenGraph2}
 		\caption{\footnotesize GoldenGraph}\label{fig:GoldenGraph}
    \end{subfigure}              
   \quad
    \begin{subfigure}[t]{0.47\textwidth}
    \centering
   	\includegraphics[scale=0.6]{figures/GoldenGraph2_probs}
 		\caption{\footnotesize Probability transitions}\label{fig:GoldenGraph_probs}
 	\end{subfigure}      
}
   \caption{Random walk on the golden graph}
   \label{fig:Golden}
 \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The limit Markov chain ${(\tildV_n)}_{n \leq 0}$}

It is easy to see that 
$$
p_n \to p_\infty := \frac{1}{1+\phi^2}= \frac{\theta}{1+\theta} = \frac{\sqrt{5}-1}{2\sqrt{5}}.
$$
and 
$$
\begin{pmatrix}
f_n/f_{n-1} & f_{n+1}/f_{n-1} \\
1 & 0
\end{pmatrix} 
\to \begin{pmatrix}
\phi^{-1} & \phi^{-2} \\
1 & 0
\end{pmatrix}. 
$$

The other Markov chain ${(\tildV_n)}_{n \leq 0}$ walking on the golden graph has law given by:

\begin{itemize}
\item $\tildV_n$ takes the value $0$ or $1$ and $\Pr(\tildV_n=1) = p_\infty$;

\item The transition matrix from $\tildV_{n}$ to $\tildV_{n+1}$ is 
$$
\begin{pmatrix}
\phi^{-1} & \phi^{-2} \\
1 & 0
\end{pmatrix}
$$
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Joining the two Markov chains}


Let $n_0 \leq 0$. We are going to define a joining of $(V_{n_0}, \ldots, V_0)$ and 
$(\tildV_{n_0}, \ldots, \tildV_0)$.

We consider the joint law of $V_{n_0}$ and $\tildV_{n_0}$ given by
$$
\begin{cases}
\begin{pmatrix} 
1-p_\infty &  p_\infty - p_{n_0} \\
0 & p_{n_0}
\end{pmatrix} & \text{if $n$ is odd} \\
\begin{pmatrix} 
1-p_{n_0}  &  0 \\
p_{n_0} - p_\infty  & p_\infty
\end{pmatrix} & \text{if $n$ is even}
\end{cases}
$$
Hence $\Pr(V_{n_0} \neq \tildV_{n_0}) = |p_{n_0}-p_\infty|$.

The transition matrix from $(V_n, \tildV_n)$ to $(V_{n+1}, \tildV_{n+1})$ 
necessarily has the following form:
$$
\begin{array}{r|cccc}
%\diagbox{$V_{n}$}{$V_{n+1}$}
 & (0,0) & (0,1) & (1,0) & (1,1) \\ 
  \hline
(0,0) & ?  &  ? & ?  & ? \\ 
  (0,1) & f_{n}/f_{n-1} & 0 & f_{n+1}/f_{n-1} & 0 \\ 
  (1,0) & \phi^{-1}  & \phi^{-2}  & 0 & 0 \\ 
  (1,1) & 1 & 0 & 0 & 0 \\ 
\end{array} 
$$
One just needs to fill the first line. One takes 
$$
\begin{cases}
\begin{array}{r|cccc}
%\diagbox{$V_{n}$}{$V_{n+1}$}
 & (0,0) & (0,1) & (1,0) & (1,1) \\ 
  \hline
(0,0) & \phi^{-1}  &  \frac{f_n}{f_{n-1}} - \phi^{-1} & 0  & \frac{f_{n+1}}{f_{n-1}}\\ 
  (0,1) & f_{n}/f_{n-1} & 0 & f_{n+1}/f_{n-1} & 0 \\ 
  (1,0) & \phi^{-1}  & \phi^{-2}  & 0 & 0 \\ 
  (1,1) & 1 & 0 & 0 & 0 \\ 
\end{array} 
& \text{if $n$ is even}
\\ \\
\begin{array}{r|cccc}
 & (0,0) & (0,1) & (1,0) & (1,1) \\ 
  \hline
(0,0) & \frac{f_n}{f_{n-1}} &  0 & \phi^{-1} - \frac{f_n}{f_{n-1}}  & \phi^{-1}\\ 
  (0,1) & f_{n}/f_{n-1} & 0 & f_{n+1}/f_{n-1} & 0 \\ 
  (1,0) & \phi^{-1}  & \phi^{-2}  & 0 & 0 \\ 
  (1,1) & 1 & 0 & 0 & 0 \\ 
\end{array}
& \text{if $n$ is odd}
\end{cases}
$$
There is a constructive way to get this joining. 
Consider a vector $(U_{n_0+1}, \ldots, U_0)$ of independent random variables having the 
uniform distribution on $(0,1)$ and which is independent of $(V_{n_0}, \tildV_{n_0})$. 
Then, for each $n$ recursively going from $n_0$ to $-1$, set 
$$
V_{n+1} = \begin{cases}
0 & \text{if $V_n=1$} \\
0 & \text{if $V_n=0$ and $U_{n+1} < \frac{f_n}{f_{n-1}}$} \\
1 & \text{if $V_n=0$ and $U_{n+1} \geq \frac{f_n}{f_{n-1}}$}
\end{cases}, \quad 
\tildV_{n+1} = \begin{cases}
0 & \text{if $\tildV_n=1$} \\
0 & \text{if $\tildV_n=0$ and $U_{n+1} < \phi^{-1}$} \\
1 & \text{if $\tildV_n=0$ and $U_{n+1} \geq \phi^{-1}$}
\end{cases}.
$$
In this way it is clear that this joining of 
$(V_{n_0}, \ldots, V_0)$ and 
$(\tildV_{n_0}, \ldots, \tildV_0)$ is synchronous. 

The probability 
$$
\beta_n:=\Pr(V_{n+1} = \tildV_{n+1} \mid V_{n}=\tildV_{n})
$$
is an average of 
$\Pr(V_{n+1} = \tildV_{n+1} \given V_{n}=\tildV_{n}=1)  = 1$ and 
$$
\Pr(V_{n+1} = \tildV_{n+1} \given V_{n}=\tildV_{n}=0) 
= 1 - \left|\frac{f_n}{f_{n-1}} - \phi^{-1}\right| =: \alpha_n, 
$$
hence it lies between $\alpha_n$ and $1$. 
Thus, when $n_0 \leq N \leq 0$, 
$$
\Pr(V_{n_0} = \tildV_{n_0}, V_{n_0+1} = \tildV_{n_0+1}, \ldots, V_{N} = \tildV_{N}) = 
\Pr(V_{n_0} = \tildV_{n_0}) \prod_{k={n_0}}^N \beta_k 
\geq \Pr(V_{n_0} = \tildV_{n_0}) \prod_{k={n_0}}^N \alpha_k.
$$
The rational numbers $\frac{f_n}{f_{n-1}}$ are the convergent of the continued fraction 
expansion of $\phi^{-1}$, hence $1 - \alpha_n \leq {(f_{n-1}f_{n-2})}^{-1}$. 
Therefore $\prod \alpha_k$ is a divergent product.

Consequently, the two processes ${(V_n)}_{n \leq 0}$ and ${(\tildV_n)}_{n \leq 0}$ 
are joinable. Indeed, given $\delta>0$, if one takes $N$ such that 
$\prod_{k={-\infty}}^N \alpha_k > 1 - \delta$ and such that 
$\Pr(V_{n_0} = \tildV_{n_0}) > 1-\delta$ when the construction starts at 
$n_0 \leq N$, one gets 
$$
\Pr(V_{n_0} = \tildV_{n_0}, V_{n_0+1} = \tildV_{n_0+1}, \ldots, V_{N} = \tildV_{N}) > 1-2\delta.
$$

\end{document}